# GPT Addition Experiment  
Exploring GPT-2's ability to learn addition by analyzing attention maps and testing different training approaches.  

For example, the attention scores of the "=" token (when predicting the next token, i.e., 3) show that it focuses heavily on the "2" from "23" and the "1" from "12," while almost completely ignoring the "3" and "2" from the respective operands.  

![attention_map](https://github.com/user-attachments/assets/284f2f13-df2b-4aaf-8a01-bd63bb89216e)  
