{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:28:35.706753Z","iopub.status.busy":"2025-02-03T14:28:35.706486Z","iopub.status.idle":"2025-02-03T14:28:58.391151Z","shell.execute_reply":"2025-02-03T14:28:58.390503Z","shell.execute_reply.started":"2025-02-03T14:28:35.706722Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2LMHeadModel, GPT2Config\n","import matplotlib.pyplot as plt\n","import ipywidgets as widgets\n","from IPython.display import display\n","from tqdm.notebook import tqdm\n","from safetensors.torch import save_model, load_model"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenizer\n","Each digit would be cosidered as a token, and to that we will add the +, =, and PAD tokens"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:29:00.817546Z","iopub.status.busy":"2025-02-03T14:29:00.817263Z","iopub.status.idle":"2025-02-03T14:29:00.823509Z","shell.execute_reply":"2025-02-03T14:29:00.822502Z","shell.execute_reply.started":"2025-02-03T14:29:00.817524Z"},"trusted":true},"outputs":[],"source":["class SimpleTokenizer:\n","    def __init__(self):\n","        self.vocab = {\n","            **{str(i): i for i in range(10)},  # Digits 0-9\n","            \"+\": 10, \"=\": 11, \"PAD\": 12\n","        }\n","        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n","\n","    def encode(self, text, max_length=9, return_tensor=True):\n","        tokens = []\n","        for char in text:\n","            if char in self.vocab:\n","                tokens.append(self.vocab[char])\n","        # Pad with \"PAD\" tokens\n","        tokens += [self.vocab[\"PAD\"]] * (max_length - len(tokens))\n","        return torch.tensor(tokens) if return_tensor else tokens\n","\n","    def decode(self, tokens):\n","        return \"\".join([self.inv_vocab[t] for t in tokens if t != self.vocab[\"PAD\"]])\n","\n","tokenizer = SimpleTokenizer()"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset and dataloader\n","\n","The dataset consists of inputs which contains the token ids of operands and the operation for additions. The numbers are each sampled from the range [0,99["]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:31:09.032871Z","iopub.status.busy":"2025-02-03T14:31:09.032547Z","iopub.status.idle":"2025-02-03T14:31:09.089831Z","shell.execute_reply":"2025-02-03T14:31:09.089189Z","shell.execute_reply.started":"2025-02-03T14:31:09.032847Z"},"trusted":true},"outputs":[],"source":["class AdditionDataset(Dataset):\n","    def __init__(self, size=10000, max_length=9):\n","        self.data = [(np.random.randint(0, 99), np.random.randint(0, 99)) for _ in range(size)]\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        a, b = self.data[idx]\n","        input_part = f\"{a}+{b}=\"  # e.g., \"12+34=\"\n","        target_part = str(a + b)   # e.g., \"46\"\n","        full_sequence = input_part + target_part  # e.g., \"12+34=46\"\n","\n","        # Tokenize the full sequence\n","        input_ids = tokenizer.encode(full_sequence, max_length=self.max_length)\n","        attention_mask = (input_ids != tokenizer.vocab[\"PAD\"]).float()\n","        return input_ids, attention_mask\n","\n","\n","dataset = AdditionDataset()\n","dataloader = DataLoader(dataset, batch_size=2048, shuffle=True, pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["## GPT2 Model init:"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:31:34.430357Z","iopub.status.busy":"2025-02-03T14:31:34.430078Z","iopub.status.idle":"2025-02-03T14:31:34.471072Z","shell.execute_reply":"2025-02-03T14:31:34.469942Z","shell.execute_reply.started":"2025-02-03T14:31:34.430336Z"},"trusted":true},"outputs":[],"source":["config = GPT2Config(\n","    vocab_size=len(tokenizer.vocab),\n","    n_positions=9,\n","    n_embd=128,\n","    n_layer=8,\n","    n_head=2\n",")\n","model = GPT2LMHeadModel(config)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:29:07.051778Z","iopub.status.busy":"2025-02-03T14:29:07.051445Z","iopub.status.idle":"2025-02-03T14:29:07.057655Z","shell.execute_reply":"2025-02-03T14:29:07.056749Z","shell.execute_reply.started":"2025-02-03T14:29:07.051752Z"},"id":"aj91NdOSKgc3","trusted":true},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n","\n","def train(model, dataloader, optimizer, epochs=10):\n","    model.train()\n","    progress_bar = tqdm(range(epochs), desc='Training', unit='epoch')\n","    \n","    for epoch in progress_bar:\n","        total_loss = 0\n","        for input_ids, attention_mask in dataloader:\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            \n","            # Forward pass\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n","            loss = outputs.loss\n","            \n","            # Backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            total_loss += loss.item()\n","        \n","        avg_loss = total_loss / len(dataloader)\n","        progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\")  # Updates tqdm bar instead of printing new lines"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:27:58.131986Z","iopub.status.busy":"2025-02-03T14:27:58.131513Z","iopub.status.idle":"2025-02-03T14:27:58.185991Z","shell.execute_reply":"2025-02-03T14:27:58.184933Z","shell.execute_reply.started":"2025-02-03T14:27:58.131947Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91357fc910f3428aacabb24f34a79af9","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/100 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train(model, dataloader, optimizer, epochs=100)\n","\n","save_model(model, \"models/model.safetensors\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(13, 128)\n","    (wpe): Embedding(9, 128)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-7): 8 x GPT2Block(\n","        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=384, nx=128)\n","          (c_proj): Conv1D(nf=128, nx=128)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=512, nx=128)\n","          (c_proj): Conv1D(nf=128, nx=512)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=128, out_features=13, bias=False)\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["load_model(model, \"models/model.safetensors\")\n","model.eval()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.status.busy":"2025-02-03T14:26:02.075135Z","iopub.status.idle":"2025-02-03T14:26:02.075382Z","shell.execute_reply":"2025-02-03T14:26:02.075282Z"},"trusted":true},"outputs":[],"source":["def predict(a, b):\n","    input_part = f\"{a}+{b}=\"\n","    input_ids = tokenizer.encode(input_part, max_length=len(input_part)).unsqueeze(0).to(device)  # Add batch dim\n","    #attention_mask = (input_ids != tokenizer.vocab[\"PAD\"]).float().to(device)\n","    print(input_ids)\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids,\n","            #attention_mask=attention_mask,\n","            max_new_tokens=4,  # Generate up to 3 digits (e.g., \"123\")\n","            pad_token_id=tokenizer.vocab[\"PAD\"]\n","        )\n","    print(outputs)\n","    generated = tokenizer.decode(outputs[0].cpu().numpy())\n","    print(generated)\n","    return generated.split(\"=\")[-1]  # Extract the generated sum"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([ 1,  2, 10,  9,  9, 11])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["input_part = '12+99='\n","tokenizer.encode(input_part, max_length=len(input_part))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:3070\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3066\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3067\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3068\u001b[0m         )\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["model.cuda()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:26:02.358255Z","iopub.status.busy":"2025-02-03T14:26:02.358013Z","iopub.status.idle":"2025-02-03T14:26:02.377121Z","shell.execute_reply":"2025-02-03T14:26:02.376004Z","shell.execute_reply.started":"2025-02-03T14:26:02.358235Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (9). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"]},{"name":"stdout","output_type":"stream","text":["tensor([[ 2,  3, 10,  1,  2, 11]])\n","tensor([[ 2,  3, 10,  1,  2, 11,  3,  5, 12, 12]])\n","23+12=35\n"]},{"data":{"text/plain":["'35'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["predict(23, 12)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6874e8ad32a84e5597341c24ed6fafb2","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Text(value='23+12=34', description='Sentence:'), IntSlider(value=1, description='Layer:', max=8…"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb6ebe2a4d9a410396f6d8c98a173c83","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# Generate output and extract attention maps\n","def visualize_attention(sentence, layer_idx, head_idx):\n","    input_ids = tokenizer.encode(sentence, return_tensor=True, max_length=9).unsqueeze(0).to(device)  # Add batch dimension\n","    attention_mask = (input_ids != tokenizer.vocab[\"PAD\"]).float().to(device)  # Create attention mask\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask, output_attentions=True)\n","\n","    attentions = torch.stack(outputs.attentions).squeeze(dim=1).cpu().numpy()  # Shape: (layers, heads, seq_len, seq_len)\n","    attn_map = attentions[layer_idx-1, head_idx-1]\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(attn_map, cmap=\"viridis\")\n","    plt.colorbar()\n","    plt.title(f\"Attention Map (Layer {layer_idx}, Head {head_idx})\")\n","    plt.xticks(range(len(sentence)), [x for x in sentence])\n","    plt.yticks(range(len(sentence)), [x for x in sentence])\n","    plt.show()\n","\n","# Interactive widgets\n","sentence_widget = widgets.Text(value=\"23+12=34\", description=\"Sentence:\")\n","layer_widget = widgets.IntSlider(value=1, min=1, max=len(model.transformer.h), step=1, description=\"Layer:\")\n","head_widget = widgets.IntSlider(value=1, min=1, max=2, step=1, description=\"Head:\")\n","\n","ui = widgets.VBox([sentence_widget, layer_widget, head_widget])\n","output = widgets.Output()\n","\n","def update_visualization(_):\n","    with output:\n","        output.clear_output(wait=True)\n","        visualize_attention(sentence_widget.value, layer_widget.value, head_widget.value)\n","\n","sentence_widget.observe(update_visualization, names='value')\n","layer_widget.observe(update_visualization, names='value')\n","head_widget.observe(update_visualization, names='value')\n","\n","display(ui, output)\n","update_visualization(None)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
