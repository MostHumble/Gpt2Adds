{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:28:35.706753Z","iopub.status.busy":"2025-02-03T14:28:35.706486Z","iopub.status.idle":"2025-02-03T14:28:58.391151Z","shell.execute_reply":"2025-02-03T14:28:58.390503Z","shell.execute_reply.started":"2025-02-03T14:28:35.706722Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2LMHeadModel, GPT2Config\n","import matplotlib.pyplot as plt\n","import ipywidgets as widgets\n","from IPython.display import display\n","from tqdm.notebook import tqdm\n","from safetensors.torch import save_model, load_model"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenizer\n","Each digit would be cosidered as a token, and to that we will add the +, =, and PAD tokens"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:29:00.817546Z","iopub.status.busy":"2025-02-03T14:29:00.817263Z","iopub.status.idle":"2025-02-03T14:29:00.823509Z","shell.execute_reply":"2025-02-03T14:29:00.822502Z","shell.execute_reply.started":"2025-02-03T14:29:00.817524Z"},"trusted":true},"outputs":[],"source":["class SimpleTokenizer:\n","    def __init__(self):\n","        self.vocab = {\n","            **{str(i): i for i in range(10)},  # Digits 0-9\n","            \"+\": 10, \"=\": 11, \"PAD\": 12\n","        }\n","        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n","\n","    def encode(self, text, max_length=9, return_tensor=True):\n","        tokens = []\n","        for char in text:\n","            if char in self.vocab:\n","                tokens.append(self.vocab[char])\n","        # Pad with \"PAD\" tokens\n","        tokens += [self.vocab[\"PAD\"]] * (max_length - len(tokens))\n","        return torch.tensor(tokens) if return_tensor else tokens\n","\n","    def decode(self, tokens):\n","        return \"\".join([self.inv_vocab[t] for t in tokens if t != self.vocab[\"PAD\"]])\n","\n","tokenizer = SimpleTokenizer()"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset and dataloader\n","\n","The dataset consists of:\n","- inputs, which contains the token ids of operands and the operation for additions. The numbers ar each sampled from in the range [0,50[ so as to have a sum < 100\n","- targets where formulated in a way that it that the model would not be trained to predict only the results (by using the -100 token"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:31:09.032871Z","iopub.status.busy":"2025-02-03T14:31:09.032547Z","iopub.status.idle":"2025-02-03T14:31:09.089831Z","shell.execute_reply":"2025-02-03T14:31:09.089189Z","shell.execute_reply.started":"2025-02-03T14:31:09.032847Z"},"trusted":true},"outputs":[],"source":["class AdditionDataset(Dataset):\n","    def __init__(self, size=10000, max_length=9):\n","        self.data = [(np.random.randint(0, 99), np.random.randint(0, 99)) for _ in range(size)]\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        a, b = self.data[idx]\n","        input_part = f\"{a}+{b}=\"  # e.g., \"12+34=\"\n","        target_part = str(a + b)   # e.g., \"46\"\n","        full_sequence = input_part + target_part  # e.g., \"12+34=46\"\n","\n","        # Tokenize the full sequence\n","        input_ids = tokenizer.encode(full_sequence, max_length=self.max_length)\n","        attention_mask = (input_ids != tokenizer.vocab[\"PAD\"]).float()\n","        return input_ids, attention_mask\n","\n","\n","dataset = AdditionDataset()\n","dataloader = DataLoader(dataset, batch_size=2048, shuffle=True, pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["## GPT2 Model init:"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:31:34.430357Z","iopub.status.busy":"2025-02-03T14:31:34.430078Z","iopub.status.idle":"2025-02-03T14:31:34.471072Z","shell.execute_reply":"2025-02-03T14:31:34.469942Z","shell.execute_reply.started":"2025-02-03T14:31:34.430336Z"},"trusted":true},"outputs":[],"source":["config = GPT2Config(\n","    vocab_size=len(tokenizer.vocab),\n","    n_positions=9,\n","    n_embd=128,\n","    n_layer=8,\n","    n_head=2\n",")\n","model = GPT2LMHeadModel(config)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:29:07.051778Z","iopub.status.busy":"2025-02-03T14:29:07.051445Z","iopub.status.idle":"2025-02-03T14:29:07.057655Z","shell.execute_reply":"2025-02-03T14:29:07.056749Z","shell.execute_reply.started":"2025-02-03T14:29:07.051752Z"},"id":"aj91NdOSKgc3","trusted":true},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n","\n","def train(model, dataloader, optimizer, epochs=10):\n","    model.train()\n","    progress_bar = tqdm(range(epochs), desc='Training', unit='epoch')\n","    \n","    for epoch in progress_bar:\n","        total_loss = 0\n","        for input_ids, attention_mask in dataloader:\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            \n","            # Forward pass\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n","            loss = outputs.loss\n","            \n","            # Backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            total_loss += loss.item()\n","        \n","        avg_loss = total_loss / len(dataloader)\n","        progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\")  # Updates tqdm bar instead of printing new lines"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:27:58.131986Z","iopub.status.busy":"2025-02-03T14:27:58.131513Z","iopub.status.idle":"2025-02-03T14:27:58.185991Z","shell.execute_reply":"2025-02-03T14:27:58.184933Z","shell.execute_reply.started":"2025-02-03T14:27:58.131947Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91357fc910f3428aacabb24f34a79af9","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/100 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train(model, dataloader, optimizer, epochs=100)\n","\n","save_model(model, \"models/model.safetensors\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(13, 128)\n","    (wpe): Embedding(9, 128)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-7): 8 x GPT2Block(\n","        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=384, nx=128)\n","          (c_proj): Conv1D(nf=128, nx=128)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=512, nx=128)\n","          (c_proj): Conv1D(nf=128, nx=512)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=128, out_features=13, bias=False)\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["load_model(model, \"models/model.safetensors\")\n","model.eval()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.status.busy":"2025-02-03T14:26:02.075135Z","iopub.status.idle":"2025-02-03T14:26:02.075382Z","shell.execute_reply":"2025-02-03T14:26:02.075282Z"},"trusted":true},"outputs":[],"source":["def predict(a, b):\n","    input_part = f\"{a}+{b}=\"\n","    input_ids = tokenizer.encode(input_part, max_length=len(input_part)).unsqueeze(0).to(device)  # Add batch dim\n","    #attention_mask = (input_ids != tokenizer.vocab[\"PAD\"]).float().to(device)\n","    print(input_ids)\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids,\n","            #attention_mask=attention_mask,\n","            max_new_tokens=4,  # Generate up to 3 digits (e.g., \"123\")\n","            pad_token_id=tokenizer.vocab[\"PAD\"]\n","        )\n","    print(outputs)\n","    generated = tokenizer.decode(outputs[0].cpu().numpy())\n","    print(generated)\n","    return generated.split(\"=\")[-1]  # Extract the generated sum"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([ 1,  2, 10,  9,  9, 11])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["input_part = '12+99='\n","tokenizer.encode(input_part, max_length=len(input_part))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(13, 128)\n","    (wpe): Embedding(9, 128)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-7): 8 x GPT2Block(\n","        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=384, nx=128)\n","          (c_proj): Conv1D(nf=128, nx=128)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=512, nx=128)\n","          (c_proj): Conv1D(nf=128, nx=512)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=128, out_features=13, bias=False)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model.cuda()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2025-02-03T14:26:02.358255Z","iopub.status.busy":"2025-02-03T14:26:02.358013Z","iopub.status.idle":"2025-02-03T14:26:02.377121Z","shell.execute_reply":"2025-02-03T14:26:02.376004Z","shell.execute_reply.started":"2025-02-03T14:26:02.358235Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 2,  3, 10,  1,  2, 11]], device='cuda:0')\n","tensor([[ 2,  3, 10,  1,  2, 11,  3,  5, 12, 12]], device='cuda:0')\n","23+12=35\n"]},{"data":{"text/plain":["'35'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["predict(23, 12)"]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d01eb2d164144cce90a2bb5c486db019","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Text(value='23+12=34', description='Sentence:'), IntSlider(value=1, description='Layer:', max=8…"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7075b437b62f488ba6553e37242929af","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["# Generate output and extract attention maps\n","def visualize_attention(sentence, layer_idx, head_idx):\n","    input_ids = tokenizer.encode(sentence, return_tensor=True, max_length=9).unsqueeze(0).to(device)  # Add batch dimension\n","    attention_mask = (input_ids != tokenizer.vocab[\"PAD\"]).float().to(device)  # Create attention mask\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask, output_attentions=True)\n","\n","    attentions = torch.stack(outputs.attentions).squeeze(dim=1).cpu().numpy()  # Shape: (layers, heads, seq_len, seq_len)\n","    attn_map = attentions[layer_idx-1, head_idx-1]\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(attn_map, cmap=\"viridis\")\n","    plt.colorbar()\n","    plt.title(f\"Attention Map (Layer {layer_idx}, Head {head_idx})\")\n","    plt.xticks(range(len(sentence)), [x for x in sentence])\n","    plt.yticks(range(len(sentence)), [x for x in sentence])\n","    plt.show()\n","\n","# Interactive widgets\n","sentence_widget = widgets.Text(value=\"23+12=34\", description=\"Sentence:\")\n","layer_widget = widgets.IntSlider(value=1, min=1, max=len(model.transformer.h), step=1, description=\"Layer:\")\n","head_widget = widgets.IntSlider(value=1, min=1, max=2, step=1, description=\"Head:\")\n","\n","ui = widgets.VBox([sentence_widget, layer_widget, head_widget])\n","output = widgets.Output()\n","\n","def update_visualization(_):\n","    with output:\n","        output.clear_output(wait=True)\n","        visualize_attention(sentence_widget.value, layer_widget.value, head_widget.value)\n","\n","sentence_widget.observe(update_visualization, names='value')\n","layer_widget.observe(update_visualization, names='value')\n","head_widget.observe(update_visualization, names='value')\n","\n","display(ui, output)\n","update_visualization(None)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
